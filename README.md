# RAG Application Evaluation Workshop

Welcome to the **RAG Application Evaluation Workshop** repo! This repository contains all materials used in the hands-on session focused on building an automated evaluation pipeline for Retrieval-Augmented Generation (RAG) applications. The workshop was presented at the DataNights Gen AI Courses #1, #2

## üìÅ Contents

- **Slides**  
  Workshop presentation slides covering key concepts, evaluation criteria, and step-by-step guidance.

- **Data**  
  Curated dataset of ~80 annotated samples used during the workshop for evaluation tasks.

- **Solution/**  
  Example implementation of an evaluation pipeline, including baseline code for measuring completeness, relevance, and hallucination detection.

## üß† What You‚Äôll Learn

- How to build an automated evaluation pipeline for RAG-based applications  
- Define and apply key evaluation criteria: completeness, relevance, hallucinations, contradictions  
- Analyze real LLM outputs and identify failure cases  
- Implement simple, testable evaluation methods with clear success metrics  
- Use LLMs and supporting models to check if answers are grounded in retrieved context  

## üì¨ Questions?

Feel free to reach out! <br>
nadavbarak148@gmail.com <br>
https://www.linkedin.com/in/nadavbarak/
